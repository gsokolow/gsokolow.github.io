I am particularly interested in your interpretation and reaction to figure 6.1 in Longley et al (2008) 
with regards to the three questions/prompts below:

Do you have first-hand knowledge or experience with uncertainty in spatial/geographic research?
What responsibilities do geographers have with regards to uncertainty in research?
What strategies might geographers use to fulfill those responsibilities?

As children, we may be taught that maps tell truths. As geographers, we are taught that maps lie.
Just like any other model, maps simplify the world around us so that we may better understand it.
Any choice we make to simplify necessarily creates the opportunity for bias and uncertainty.
In Chapter 6 of **Geographic information systems and science** (Langley et al, 2008), 
the following figure is presented as a 'conceptual view of uncertainty.
The three filters, U1, U2, and U3 can distort the way in which the complexity of the real world is conceived, 
measured, represented, and analyzed in a cumulative way.'
![Conceptual Model of Uncertainty](file:///Users/gracesokolow/Desktop/Open%20Source/Screenshot%202023-10-09%20at%208.14.31%20PM.png "Longley et. al, 2008. Figure 1")

This figure represents how uncertainty can manifest at every stage of abstraction and analysis.
It begins to describe the ways in which these errors propogate through the course of an analysis,
influencing all subsequent decisions and calculations in it.
However, it could do a better job visually representing the potential for errors to magnify, or at least, to interact. 
If the size of the purple wave ast any stage is related to the magnitude of uncertainty at that stage in the analysis,
then the current configuration suggests that the risk of uncertainty grows at the stages of conception and measurement, 
but shrinks at the final stage of analysis.
A radial model, in which total uncertainty grows as a function of progression through the analysis, might be able to make this figure even better.

In another course I took at Middlebury (Remote Sensing & Land Use with Niwaeli Kimambo, Fall 2022), 
we used machine learning to classify land use types from satellite imagery of Madagascar. 
We started by defining our land cover classes, which is where we encountered conceptual uncertainty.
None of the students had visited our study site, so we were at a loss when it came to defining the different land cover classes in ways that 
were meaningfully differentiated and functionally descriptive. 
Using our impefect class definitions, we generated a set of training points. 
We input the training points into our machine learning model to communicate the characteristics we associated with each class.
In this way, any uncertainty from **our** class definition was carried through to the model, where it likely interacted with uncertainty inherent in the model's operation.
Once we used our model to classify the land cover types in our study region, we conducted an error analysis.
We generated a stratified sample of points in each land class and manually classified them independent of the model's classification.
This gave us a sense of the uncertainty in our classification scheme, but remained imperfect due to the conceptual uncertainty underlying the project.
Even when we were manually classifying points, our limited understanding of the landscape introduced uncertainty into the 'ground truth' points we used to measure our model's error.
Similarly, the stratified sample we took in order to generate these ground-truthing points was based on our limited understanding of land cover in the region, 
and was thus influenced by the very uncertainty we were using them to understand. 
This internal dependency feels a little like using a word you are trying to define in it's definition. 
It doesn't give a very full picture.


by creating training points, which we gave to our model to tell it the characteristics associated with each land use class.
.. but fails to adequately show how it can magnify?
